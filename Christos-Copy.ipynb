{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f147e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower, when, col, to_timestamp, year, rank, count, isnan, avg, sum\n",
    "from pyspark.sql.window import Window\n",
    "from sedona.spark import SedonaContext, ST_Point, ST_Within, ST_Distance\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Execution time for {func.__name__}: {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fdfa12-5703-4924-9e04-7e660e923462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"LosAngelesCrime\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7689a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session \\\n",
    "    .read \\\n",
    "    .csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc4202",
   "metadata": {},
   "source": [
    "Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4862137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Date Rptd\", to_timestamp(\"Date Rptd\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "df = df.withColumn(\"DATE OCC\", to_timestamp(\"DATE OCC\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "df = df.withColumnRenamed(\"AREA \", \"AREA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a697e6",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262463e",
   "metadata": {},
   "source": [
    "## Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745247ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def q1_dataframe(df):\n",
    "    df \\\n",
    "    .filter(df[\"Crm Cd Desc\"].contains(\"AGGRAVATED ASSAULT\")) \\\n",
    "    .withColumn(\n",
    "        \"Vict Age Group\",\n",
    "        when(col(\"Vict Age\") < 18, \"<18\")\n",
    "        .when((18 <= col(\"Vict Age\")) & (col(\"Vict Age\") <= 24), \"18-24\")\n",
    "        .when((25 <= col(\"Vict Age\")) & (col(\"Vict Age\") <= 64), \"25-64\")\n",
    "        .when(64 < col(\"Vict Age\"), \">64\")) \\\n",
    "    .groupBy(\"Vict Age Group\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .show()\n",
    "\n",
    "q1_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d2e1a",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58afbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_group(row):\n",
    "    if int(row['Vict Age']) < 18:\n",
    "        return \"<18\"\n",
    "    elif 18 <= int(row['Vict Age']) <= 24:\n",
    "        return \"18-24\"\n",
    "    elif 25 <= int(row['Vict Age']) <= 64:\n",
    "        return \"25-64\"\n",
    "    else:\n",
    "        return \">64\"\n",
    "\n",
    "@timed\n",
    "def q1_rdd(df):    \n",
    "    df \\\n",
    "        .rdd \\\n",
    "        .filter(lambda x: \"AGGRAVATED ASSAULT\" in x[\"Crm Cd Desc\"]) \\\n",
    "        .map(lambda x: (age_group(x), 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .sortBy(lambda x: -x[1]) \\\n",
    "        .toDF([\"Vict Age Group\", \"count\"]) \\\n",
    "        .show()\n",
    "q1_rdd(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Status Desc\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35052d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_indicator(row):\n",
    "    if row[\"Status Desc\"] in (\"Invest Cont\", \"UNK\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abc630",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2267c5b6",
   "metadata": {},
   "source": [
    "## α)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea5308c",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb15bfc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@timed\n",
    "def query2_rdd(df):\n",
    "    df \\\n",
    "    .rdd \\\n",
    "    .map(lambda row: ((row[\"AREA NAME\"], row[\"DATE OCC\"].year), (processed_indicator(row), 1))) \\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "    .mapValues(lambda x: x[0] / x[1]) \\\n",
    "    .map(lambda x: (x[0][1], x[0][0], x[1])) \\\n",
    "    .toDF([\n",
    "        \"year\",\n",
    "        \"precinct\",\n",
    "        \"closed_case_rate\",\n",
    "    ]) \\\n",
    "    .withColumn(\"#\", rank().over(Window.partitionBy(\"year\").orderBy(col(\"closed_case_rate\").desc()))) \\\n",
    "    .orderBy(\"year\", \"#\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4a3558",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2_rdd(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de12238",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def query2_sql(spark_session, df):\n",
    "    df.createOrReplaceTempView(\"crime_data\")\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        RANK() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) as `#`\n",
    "    FROM (\n",
    "        SELECT \n",
    "            YEAR(`DATE OCC`) as year,\n",
    "            `AREA NAME` as precinct,\n",
    "            COUNT(CASE WHEN `Status Desc` NOT IN ('Invest Cont', 'UNK') THEN 1 END) / COUNT(*) as closed_case_rate\n",
    "        FROM crime_data\n",
    "        GROUP BY `AREA NAME`, YEAR(`DATE OCC`)\n",
    "    )\n",
    "    \"\"\"\n",
    "    result = spark_session.sql(query)\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a07c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2_sql(spark_session, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5cd67",
   "metadata": {},
   "source": [
    "## β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eefdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .repartition(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3://groups-bucket-dblab-905418150721/group3/crime_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b91a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_loaded_df = spark_session \\\n",
    "    .read \\\n",
    "    .parquet(\"s3://groups-bucket-dblab-905418150721/group3/crime_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d34440",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parquet_loaded_df = parquet_loaded_df.withColumn(\"Date Rptd\", to_timestamp(\"Date Rptd\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "parquet_loaded_df = parquet_loaded_df.withColumn(\"DATE OCC\", to_timestamp(\"DATE OCC\", \"MM/dd/yyyy hh:mm:ss a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e81c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2_rdd(parquet_loaded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a01211",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8eb563",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def q3_sql(spark_session, income_df, census_df):\n",
    "    income_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a884696",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df = spark_session \\\n",
    "    .read \\\n",
    "    .csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73341c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_block_fields = spark_session \\\n",
    "    .read \\\n",
    "    .csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks_fields.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140937a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_block_fields.show(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona = SedonaContext.create(spark_session)\n",
    "census_blocks_df = sedona \\\n",
    "    .read \\\n",
    "    .format(\"geojson\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\") \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "census_blocks_df = census_blocks_df.select([col(f\"properties.{col_name}\").alias(col_name) for col_name in census_blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "    .drop(\"properties\") \\\n",
    "    .drop(\"type\")\n",
    "census_blocks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a925683",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = set(map(lambda row: row[\"COMM\"], census_blocks_df.select(\"COMM\").distinct().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee55d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_zip_codes = set(map(lambda row: row[\"ZCTA10\"], census_blocks_df.select(\"ZCTA10\").distinct().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e822fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_zip_codes = set(map(lambda row: row[\"Zip Code\"], income_df.select(\"Zip Code\").distinct().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24395426",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_zip_codes - census_zip_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662806ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(census_zip_codes - income_zip_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01055917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0954ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08507895",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_comms = set(map(lambda row: row[\"AREA NAME\"], df.select(\"AREA NAME\").distinct().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b32679",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_blocks_df.select(\"geometry\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def query_3_dataframe(crime_data_df, census_blocks_df, income_df):\n",
    "    crime_data_df = crime_data_df.withColumn(\"geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "    joined_df = crime_data_df \\\n",
    "        .join(census_blocks_df, ST_Within(crime_data_df[\"geom\"], census_blocks_df[\"geometry\"])) \\\n",
    "        .groupBy(\"COMM\") \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"crime_count\")\n",
    "    comm_population = census_blocks_df.groupBy(\"COMM\").agg(sum(\"POP_2010\").alias(\"population\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e6782",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query_3_dataframe(df, census_blocks_df, income_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"LOCATION\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d32b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_blocks_df.filter(isnan(col(\"POP_2010\"))).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22dd38a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q4 : Racial Profile Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13fb922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkConf, SparkContext, SparkSession\n",
    "from pyspark.sql.functions import col, count, when, avg\n",
    "import time\n",
    "\n",
    "# Define timed decorator\n",
    "def timed(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Execution time for {func.__name__}: {execution_time:.4f} seconds\")\n",
    "        return result, execution_time\n",
    "    return wrapper\n",
    "\n",
    "def log_progress(message):\n",
    "    print(f\"[INFO] {message}\")\n",
    "    \n",
    "# Define configurations\n",
    "configurations = [\n",
    "    {\"app_name\": \"Q4_CONFIG_1\", \"num_executors\": 2, \"executor_cores\": 4, \"executor_memory\": \"8G\"},\n",
    "    {\"app_name\": \"Q4_CONFIG_2\", \"num_executors\": 4, \"executor_cores\": 2, \"executor_memory\": \"4G\"},\n",
    "    {\"app_name\": \"Q4_CONFIG_3\", \"num_executors\": 8, \"executor_cores\": 1, \"executor_memory\": \"2G\"},\n",
    "]\n",
    "\n",
    "# Initialize a shared SparkContext\n",
    "conf = SparkConf().setAppName(\"SharedSparkContext\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "@timed\n",
    "def compute_racial_profile(query3_result_df, crime_2015_df, race_codes_df, spark_session):\n",
    "    log_progress(\"Running Query 4\")\n",
    "\n",
    "    # Top 3 and bottom 3 areas by income\n",
    "    top_3_areas = query3_result_df.orderBy(col(\"income_per_person\").desc()).limit(3)\n",
    "    bottom_3_areas = query3_result_df.orderBy(col(\"income_per_person\").asc()).limit(3)\n",
    "\n",
    "    # Join crime data with top and bottom 3 areas\n",
    "    crimes_top3 = crime_2015_df.join(top_3_areas, \"COMM\", \"inner\")\n",
    "    crimes_bottom3 = crime_2015_df.join(bottom_3_areas, \"COMM\", \"inner\")\n",
    "\n",
    "    # Map racial codes for top 3\n",
    "    crimes_top3 = crimes_top3.join(race_codes_df, crimes_top3[\"Vict Descent\"] == race_codes_df[\"DescentCode\"], \"left\")\n",
    "\n",
    "    # Map racial codes for bottom 3\n",
    "    crimes_bottom3 = crimes_bottom3.join(race_codes_df, crimes_bottom3[\"Vict Descent\"] == race_codes_df[\"DescentCode\"], \"left\")\n",
    "\n",
    "    # Group and sort for top and bottom 3\n",
    "    top3_racial_profile = crimes_top3.groupBy(\"FullDescription\").agg(count(\"*\").alias(\"victim_count\")).orderBy(col(\"victim_count\").desc())\n",
    "    bottom3_racial_profile = crimes_bottom3.groupBy(\"FullDescription\").agg(count(\"*\").alias(\"victim_count\")).orderBy(col(\"victim_count\").desc())\n",
    "\n",
    "    # Return results\n",
    "    return top3_racial_profile, bottom3_racial_profile\n",
    "\n",
    "@timed\n",
    "def run_query4_benchmark(query3_result_df, crime_2015_df, race_codes_df):\n",
    "    benchmark_results = []\n",
    "    for config in configurations:\n",
    "        log_progress(f\"Starting with SparkSession: {config['app_name']}\")\n",
    "\n",
    "        # Create a new SparkSession using the shared SparkContext and apply resource configurations\n",
    "        spark_session = (\n",
    "            SparkSession(sc)\n",
    "            .newSession()\n",
    "            .builder\n",
    "            .appName(config[\"app_name\"])\n",
    "            .config(\"spark.executor.instances\", config[\"num_executors\"])\n",
    "            .config(\"spark.executor.cores\", config[\"executor_cores\"])\n",
    "            .config(\"spark.executor.memory\", config[\"executor_memory\"])\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "        # Compute racial profile with the current configuration\n",
    "        (top3_racial_profile, bottom3_racial_profile), execution_time = compute_racial_profile(query3_result_df, crime_2015_df, race_codes_df, spark_session)\n",
    "        benchmark_results.append({\"config\": config, \"execution_time\": execution_time})\n",
    "\n",
    "        # Display results for each configuration\n",
    "        print(f\"=== Results for Config: {config['app_name']} ===\")\n",
    "        print(\"== Racial profile for top 3 high-income areas ==\")\n",
    "        top3_racial_profile.show()\n",
    "\n",
    "        print(\"== Racial profile for bottom 3 low-income areas ==\")\n",
    "        bottom3_racial_profile.show()\n",
    "\n",
    "    # Display benchmark results\n",
    "    log_progress(\"Benchmark Results:\")\n",
    "    for result in benchmark_results:\n",
    "        print(f\"Config: {result['config']}, Execution Time: {result['execution_time']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "run_query4_benchmark(query3_result_df, crime_2015_df, race_codes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e151d803",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q5: Crime Proximity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410129b",
   "metadata": {},
   "source": [
    "This query computes the closest police station for each crime, aggregates the results by station, and displays statistics such as the total number of crimes and the average distance.\n",
    "\n",
    "#### Workflow\n",
    "1. Loads police station data and crime data.\n",
    "2. Filters valid crime records.\n",
    "3. Computes distances between crimes and police stations using Apache Sedona.\n",
    "4. Finds the closest station for each crime using a window function.\n",
    "5. Aggregates crimes by station and calculates average distances.\n",
    "\n",
    "#### Output\n",
    "- `division`: Police station name.\n",
    "- `average_distance`: Average distance of crimes from the station.\n",
    "- `crime_count`: Total crimes associated with each station.\n",
    "\n",
    "The results are displayed in `descending` order of crime count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f452f",
   "metadata": {},
   "source": [
    "### Q5 Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5dba2a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 4260 did not start up in 60 seconds..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, avg, rank\n",
    "from pyspark.sql import Window\n",
    "from sedona.spark import SedonaContext, ST_Point, ST_Distance\n",
    "import time\n",
    "\n",
    "# Utility Functions\n",
    "def log_progress(message):\n",
    "    print(f\"[INFO] {message}\")\n",
    "\n",
    "def timed(func):\n",
    "    \"\"\"\n",
    "    Decorator to measure execution time of a function.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"\\nExecution time for {func.__name__}: {execution_time:.4f} seconds\\n\")\n",
    "        return result, execution_time\n",
    "    return wrapper\n",
    "\n",
    "@timed\n",
    "def run_query_5(spark, crime_df):\n",
    "    \"\"\"\n",
    "    Executes Query 5: Computes the closest police station for each crime, aggregates\n",
    "    results by station, and returns the final DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - spark: The active Spark session.\n",
    "    - crime_df: The crime DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - query5_result_df: A DataFrame containing aggregated results.\n",
    "    \"\"\"\n",
    "    log_progress(\"Initializing Sedona context\")\n",
    "    sedona = SedonaContext.create(spark)\n",
    "\n",
    "    # Load police station data\n",
    "    log_progress(\"Loading police station data\")\n",
    "    police_stations_df = spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\",\n",
    "        header=True, inferSchema=True\n",
    "    )\n",
    "    police_stations_df = police_stations_df.withColumn(\"station_geom\", ST_Point(\"X\", \"Y\"))\n",
    "\n",
    "    # Clean crime data\n",
    "    log_progress(\"Filtering valid crime data\")\n",
    "    crime_df = crime_df.filter((col(\"LAT\").isNotNull()) & (col(\"LON\").isNotNull()))\n",
    "    crime_df = crime_df.withColumn(\"crime_geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "    # Cross join crimes with police stations to compute distances\n",
    "    log_progress(\"Computing distances between crimes and police stations\")\n",
    "    distances_df = crime_df.crossJoin(police_stations_df) \\\n",
    "        .withColumn(\"distance\", ST_Distance(col(\"crime_geom\"), col(\"station_geom\")))\n",
    "\n",
    "    # Find the closest police station for each crime\n",
    "    log_progress(\"Finding the closest police station for each crime\")\n",
    "    window_spec = Window.partitionBy(\"DR_NO\").orderBy(col(\"distance\"))\n",
    "    closest_stations_df = distances_df.withColumn(\"rank\", rank().over(window_spec)).filter(col(\"rank\") == 1)\n",
    "\n",
    "    # Aggregate results by police station\n",
    "    log_progress(\"Aggregating results by police station\")\n",
    "    query5_result_df = closest_stations_df.groupBy(\"DIVISION\") \\\n",
    "        .agg(\n",
    "            count(\"DR_NO\").alias(\"crime_count\"),\n",
    "            avg(\"distance\").alias(\"average_distance\")\n",
    "        ) \\\n",
    "        .orderBy(col(\"crime_count\").desc())\n",
    "\n",
    "    # Display results\n",
    "    log_progress(\"Displaying Query 5 results\")\n",
    "    query5_result_df.select(\n",
    "        col(\"DIVISION\").alias(\"division\"),\n",
    "        col(\"average_distance\"),\n",
    "        col(\"crime_count\").alias(\"#\")\n",
    "    ).orderBy(col(\"#\").desc()).show()\n",
    "\n",
    "    return query5_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c7162",
   "metadata": {},
   "source": [
    "### One-by-One execution\n",
    "\n",
    "Modify the configuration object to test different Spark resource settings for executors, cores, and memory.\n",
    "\n",
    "#### Configuration Object\n",
    "\n",
    "```python\n",
    "config = {\n",
    "    \"num_executors\": 8,\n",
    "    \"executor_cores\": 1,\n",
    "    \"executor_memory\": \"2G\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c58646",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 4260 did not start up in 60 seconds..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark configuration : Change the object below to test different configs.\n",
    "config = {\n",
    "    \"num_executors\": 8,\n",
    "    \"executor_cores\": 1,\n",
    "    \"executor_memory\": \"2G\"\n",
    "}\n",
    "\n",
    "spark_session = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Q5\")\n",
    "    .config(\"spark.executor.instances\", config[\"num_executors\"])\n",
    "    .config(\"spark.executor.cores\", config[\"executor_cores\"])\n",
    "    .config(\"spark.executor.memory\", config[\"executor_memory\"])\n",
    "    .getOrCreate()\n",
    ")\n",
    "log_progress(f\"Starting with configuration: {config}\")\n",
    "\n",
    "crime_df = spark_session.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "log_progress(\"/CrimeData loaded successfully.\")\n",
    "\n",
    "# Execute Query 5\n",
    "query5_result_df, execution_time = run_query_5(spark_session, crime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a772b9",
   "metadata": {},
   "source": [
    "### Dynamic SparkSession Configuration\n",
    "\n",
    "This script benchmarks Query 5 execution under three different Spark configurations by creating new Spark sessions for each setup.\n",
    "\n",
    "#### Workflow\n",
    "1. Initializes a shared `SparkContext`.\n",
    "2. Dynamically creates a `SparkSession` for each configuration (`executors`, `cores`, and `memory`).\n",
    "3. Loads the crime dataset.\n",
    "4. Executes Query 5 and measures execution time for each configuration.\n",
    "\n",
    "#### Configurations\n",
    "1. Config 1: 2 executors, 4 cores, 8 GB memory.\n",
    "2. Config 2: 4 executors, 2 cores, 4 GB memory.\n",
    "3. Config 3: 8 executors, 1 core, 2 GB memory.\n",
    "\n",
    "#### Output\n",
    "For each configuration:\n",
    "- Displays Query 5 results.\n",
    "- Logs execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The Spark session does not have enough YARN resources to start. \n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a shared SparkContext\n",
    "conf = SparkConf().setAppName(\"SharedSparkContext\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "# Workflow for each configuration\n",
    "configs = [\n",
    "    {\"app_name\": \"Q5_CONFIG_1\", \"num_executors\": 2, \"executor_cores\": 4, \"executor_memory\": \"8G\"},\n",
    "    {\"app_name\": \"Q5_CONFIG_2\", \"num_executors\": 4, \"executor_cores\": 2, \"executor_memory\": \"4G\"},\n",
    "    {\"app_name\": \"Q5_CONFIG_3\", \"num_executors\": 8, \"executor_cores\": 1, \"executor_memory\": \"2G\"},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    log_progress(f\"Starting with SparkSession: {config['app_name']}\")\n",
    "    \n",
    "    # Create a new SparkSession using the shared SparkContext and apply resource configurations\n",
    "    spark_session = (\n",
    "        SparkSession(sc)\n",
    "        .newSession()\n",
    "        .builder\n",
    "        .appName(config[\"app_name\"])\n",
    "        .config(\"spark.executor.instances\", config[\"num_executors\"])\n",
    "        .config(\"spark.executor.cores\", config[\"executor_cores\"])\n",
    "        .config(\"spark.executor.memory\", config[\"executor_memory\"])\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    # Load the crime dataset\n",
    "    crime_df = spark_session.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    log_progress(\"/CrimeData loaded successfully.\")\n",
    "    \n",
    "    # Execute Query 5 with timing\n",
    "    query5_result_df, execution_time = run_query_5(spark_session, crime_df)\n",
    "    print(f\"Query 5 execution time for {config['app_name']}: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d714f",
   "metadata": {},
   "source": [
    "## WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88799226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# Utility Functions\n",
    "def log_progress(message):\n",
    "    print(f\"[INFO] {message}\")\n",
    "\n",
    "def timed(func):\n",
    "    \"\"\"\n",
    "    Decorator to measure execution time of a function.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"\\nExecution time for {func.__name__}: {execution_time:.4f} seconds\\n\")\n",
    "        return result, execution_time\n",
    "    return wrapper\n",
    "\n",
    "@timed\n",
    "def run_query_5(config):\n",
    "    \"\"\"\n",
    "    Runs Query 5 for a given configuration.\n",
    "    Parameters:\n",
    "    - config: Dictionary containing Spark session settings.\n",
    "    \"\"\"\n",
    "    log_progress(f\"Starting with SparkSession: {config['app_name']}\")\n",
    "\n",
    "    # Create SparkSession with the configuration\n",
    "    spark_session = (\n",
    "        SparkSession.builder\n",
    "        .appName(config[\"app_name\"])\n",
    "        .config(\"spark.executor.instances\", config[\"num_executors\"])\n",
    "        .config(\"spark.executor.cores\", config[\"executor_cores\"])\n",
    "        .config(\"spark.executor.memory\", config[\"executor_memory\"])\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    # Load the crime dataset\n",
    "    log_progress(\"Loading crime dataset...\")\n",
    "    crime_df = spark_session.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    log_progress(\"Crime data loaded successfully.\")\n",
    "\n",
    "    # Simulate Query 5 execution (replace this with your query logic)\n",
    "    log_progress(f\"Executing Query 5 for {config['app_name']}...\")\n",
    "    time.sleep(2)  # Simulated execution time\n",
    "    log_progress(f\"Query 5 completed for {config['app_name']}.\")\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark_session.stop()\n",
    "    log_progress(f\"Stopped SparkSession for {config['app_name']}.\")\n",
    "\n",
    "# Configurations for each SparkSession\n",
    "configs = [\n",
    "    {\"app_name\": \"Q5_CONFIG_1\", \"num_executors\": 2, \"executor_cores\": 4, \"executor_memory\": \"8G\"},\n",
    "    {\"app_name\": \"Q5_CONFIG_2\", \"num_executors\": 4, \"executor_cores\": 2, \"executor_memory\": \"4G\"},\n",
    "    {\"app_name\": \"Q5_CONFIG_3\", \"num_executors\": 8, \"executor_cores\": 1, \"executor_memory\": \"2G\"},\n",
    "]\n",
    "\n",
    "# Run configurations in parallel using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=len(configs)) as executor:\n",
    "    futures = [executor.submit(run_query_5, config) for config in configs]\n",
    "    for future in futures:\n",
    "        future.result()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
