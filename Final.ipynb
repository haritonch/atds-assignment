{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db961b4f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb01cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, rank, count, sum as pyspark_sum, to_timestamp, regexp_replace, udf, broadcast, avg\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "from sedona.spark import ST_Point, ST_Within, ST_Union_Aggr, ST_Distance\n",
    "from sedona.spark import SedonaContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from sedona.register import SedonaRegistrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd206f",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cabed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"Execution time for {func.__name__}: {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_crime_data(crime_data_df):\n",
    "        crime_data_df = crime_data_df.withColumn(\"Date Rptd\", to_timestamp(\n",
    "            \"Date Rptd\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "        crime_data_df = crime_data_df.withColumn(\"DATE OCC\", to_timestamp(\n",
    "            \"DATE OCC\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "        crime_data_df = crime_data_df.withColumnRenamed(\"AREA \", \"AREA\")\n",
    "        return crime_data_df\n",
    "\n",
    "def load_crime_data_df(spark_session):\n",
    "    return cleanup_crime_data(spark_session \\\n",
    "        .read \\\n",
    "        .csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/\", header=True, inferSchema=True))\n",
    "    \n",
    "    \n",
    "def load_crime_data_from_single_parquet_file(spark_session):\n",
    "    return cleanup_crime_data(spark_session \\\n",
    "        .read \\\n",
    "        .parquet(\"s3://groups-bucket-dblab-905418150721/group3/crime_data.parquet\", header=True, inferSchema=True))\n",
    "\n",
    "    \n",
    "def load_census_blocks_df(spark_session):\n",
    "    sedona = SedonaContext.create(spark_session)\n",
    "    census_blocks_df = sedona \\\n",
    "        .read \\\n",
    "        .format(\"geojson\") \\\n",
    "        .option(\"multiline\", \"true\") \\\n",
    "        .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\") \\\n",
    "        .selectExpr(\"explode(features) as features\") \\\n",
    "        .select(\"features.*\")\n",
    "    census_blocks_df = census_blocks_df.select([col(f\"properties.{col_name}\").alias(col_name) for col_name in census_blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "        .drop(\"properties\") \\\n",
    "        .drop(\"type\")\n",
    "    return census_blocks_df\n",
    "\n",
    "def load_income_df(spark_session):\n",
    "    income_df = spark_session \\\n",
    "        .read \\\n",
    "        .csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "    income_df = income_df \\\n",
    "        .withColumn(\n",
    "            \"Estimated Median Income\",\n",
    "            regexp_replace(col(\"Estimated Median Income\"),\n",
    "                           \"[$,]\", \"\").cast(\"int\")\n",
    "        )\n",
    "    return income_df\n",
    "\n",
    "def load_re_codes_df(spark_session):\n",
    "    return spark_session \\\n",
    "        .read \\\n",
    "        .csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "def load_police_stations_df(spark_session):\n",
    "    return spark_session \\\n",
    "        .read \\\n",
    "        .csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce00be9",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def query1_dataframe(crime_data_df):\n",
    "    crime_data_df \\\n",
    "        .filter(crime_data_df[\"Crm Cd Desc\"].contains(\"AGGRAVATED ASSAULT\")) \\\n",
    "        .withColumn(\n",
    "            \"Vict Age Group\",\n",
    "            when(col(\"Vict Age\") < 18, \"<18\")\n",
    "            .when((18 <= col(\"Vict Age\")) & (col(\"Vict Age\") <= 24), \"18-24\")\n",
    "            .when((25 <= col(\"Vict Age\")) & (col(\"Vict Age\") <= 64), \"25-64\")\n",
    "            .when(64 < col(\"Vict Age\"), \">64\")) \\\n",
    "        .groupBy(\"Vict Age Group\") \\\n",
    "        .count() \\\n",
    "        .orderBy(\"count\", ascending=False) \\\n",
    "        .show()\n",
    "\n",
    "\n",
    "@timed\n",
    "def query1_rdd(crime_data_df):\n",
    "    def age_group(row):\n",
    "        if int(row[\"Vict Age\"]) < 18:\n",
    "            return \"<18\"\n",
    "        elif 18 <= int(row['Vict Age']) <= 24:\n",
    "            return \"18-24\"\n",
    "        elif 25 <= int(row['Vict Age']) <= 64:\n",
    "            return \"25-64\"\n",
    "        else:\n",
    "            return \">64\"\n",
    "\n",
    "    crime_data_df \\\n",
    "        .rdd \\\n",
    "        .filter(lambda x: \"AGGRAVATED ASSAULT\" in x[\"Crm Cd Desc\"]) \\\n",
    "        .map(lambda x: (age_group(x), 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .sortBy(lambda x: -x[1]) \\\n",
    "        .toDF([\"Vict Age Group\", \"count\"]) \\\n",
    "        .show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1771500",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def query2_sql(spark_session, crime_data_df):\n",
    "    crime_data_df.createOrReplaceTempView(\"crime_data\")\n",
    "    query = \"\"\"\n",
    "        SELECT\n",
    "            year,\n",
    "            precinct,\n",
    "            closed_case_rate,\n",
    "            RANK() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) as `#`\n",
    "        FROM (\n",
    "            SELECT \n",
    "                YEAR(`DATE OCC`) as year,\n",
    "                `AREA NAME` as precinct,\n",
    "                COUNT(CASE WHEN `Status Desc` NOT IN ('Invest Cont', 'UNK') THEN 1 END) / COUNT(*) as closed_case_rate\n",
    "            FROM crime_data\n",
    "            GROUP BY `AREA NAME`, YEAR(`DATE OCC`)\n",
    "        )\n",
    "    \"\"\"\n",
    "    result = spark_session.sql(query)\n",
    "    result.show()\n",
    "\n",
    "\n",
    "@timed\n",
    "def query2_rdd(crime_data_df):\n",
    "    def processed_indicator(row):\n",
    "        if row[\"Status Desc\"] in (\"Invest Cont\", \"UNK\"):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    crime_data_df \\\n",
    "        .rdd \\\n",
    "        .map(lambda row: ((row[\"AREA NAME\"], row[\"DATE OCC\"].year), (processed_indicator(row), 1))) \\\n",
    "        .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "        .mapValues(lambda x: x[0] / x[1]) \\\n",
    "        .map(lambda x: (x[0][1], x[0][0], x[1])) \\\n",
    "        .toDF([\n",
    "            \"year\",\n",
    "            \"precinct\",\n",
    "            \"closed_case_rate\",\n",
    "        ]) \\\n",
    "        .withColumn(\"#\", rank().over(Window.partitionBy(\"year\").orderBy(col(\"closed_case_rate\").desc()))) \\\n",
    "        .orderBy(\"year\", \"#\") \\\n",
    "        .show()\n",
    "\n",
    "\n",
    "def save_crime_data_to_single_parquet_file(crime_data_df):\n",
    "    crime_data_df \\\n",
    "        .repartition(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"s3://groups-bucket-dblab-905418150721/group3/crime_data.parquet\")\n",
    "\n",
    "\n",
    "def load_crime_data_from_single_parquet_file(spark_session):\n",
    "    return spark_session \\\n",
    "        .read \\\n",
    "        .parquet(\"s3://groups-bucket-dblab-905418150721/group3/crime_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc66a818",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de11843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def query3(census_blocks_df, income_df, crime_data_df):\n",
    "    la_census_comms = census_blocks_df \\\n",
    "        .filter(col(\"CITY\") == \"Los Angeles\") \\\n",
    "        .groupBy(\"ZCTA10\", \"COMM\") \\\n",
    "        .agg(\n",
    "            ST_Union_Aggr(\"geometry\").alias(\"geometry\"),\n",
    "            pyspark_sum(\"HOUSING10\").alias(\"housing\"),\n",
    "            pyspark_sum(\"POP_2010\").alias(\"population\")\n",
    "        )\n",
    "\n",
    "    community_income_df = la_census_comms \\\n",
    "        .join(income_df, la_census_comms[\"ZCTA10\"] == income_df[\"Zip Code\"]) \\\n",
    "        .withColumn(\"income\", col(\"housing\") * col(\"Estimated Median Income\")) \\\n",
    "        .groupBy(\"COMM\") \\\n",
    "        .agg(\n",
    "            ST_Union_Aggr(\"geometry\").alias(\"comm_geometry\"),\n",
    "            pyspark_sum(\"population\").alias(\"comm_population\"),\n",
    "            pyspark_sum(\"income\").alias(\"comm_income\")\n",
    "        )\n",
    "\n",
    "    crime_data_df = crime_data_df.withColumn(\"geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "    community_income_and_crime_df = community_income_df \\\n",
    "        .join(crime_data_df, ST_Within(crime_data_df[\"geom\"], community_income_df[\"comm_geometry\"])) \\\n",
    "        .groupBy(\"COMM\", \"comm_population\", \"comm_income\") \\\n",
    "        .agg(\n",
    "            count(col(\"*\")).alias(\"#crimes\"),\n",
    "        ) \\\n",
    "        .withColumn(\"#Crimes per capita\", col(\"#crimes\") / col(\"comm_population\")) \\\n",
    "        .withColumn(\"Income per capita\", col(\"comm_income\") / col(\"comm_population\")) \\\n",
    "        .select(\"COMM\", \"#Crimes per capita\", \"Income per capita\") \\\n",
    "        .orderBy(col(\"Income per capita\").desc())\n",
    "\n",
    "    community_income_and_crime_df.show()\n",
    "    return community_income_and_crime_df\n",
    "\n",
    "@timed\n",
    "def query3_with_strategies(\n",
    "        census_blocks_df,\n",
    "        income_df,\n",
    "        crime_data_df,\n",
    "        community_income_strategy,\n",
    "        community_income_and_crime_strategy):\n",
    "    la_census_comms = census_blocks_df \\\n",
    "        .filter(col(\"CITY\") == \"Los Angeles\") \\\n",
    "        .groupBy(\"ZCTA10\", \"COMM\") \\\n",
    "        .agg(\n",
    "            ST_Union_Aggr(\"geometry\").alias(\"geometry\"),\n",
    "            pyspark_sum(\"HOUSING10\").alias(\"housing\"),\n",
    "            pyspark_sum(\"POP_2010\").alias(\"population\")\n",
    "        )\n",
    "\n",
    "    community_income_df = la_census_comms \\\n",
    "        .join(income_df.hint(community_income_strategy), la_census_comms[\"ZCTA10\"] == income_df[\"Zip Code\"]) \\\n",
    "        .hint(\"\") \\\n",
    "        .withColumn(\"income\", col(\"housing\") * col(\"Estimated Median Income\")) \\\n",
    "        .groupBy(\"COMM\") \\\n",
    "        .agg(\n",
    "            ST_Union_Aggr(\"geometry\").alias(\"comm_geometry\"),\n",
    "            pyspark_sum(\"population\").alias(\"comm_population\"),\n",
    "            pyspark_sum(\"income\").alias(\"comm_income\")\n",
    "        )\n",
    "\n",
    "    crime_data_df = crime_data_df.withColumn(\"geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "    community_income_and_crime_df = community_income_df \\\n",
    "        .join(crime_data_df.hint(community_income_and_crime_strategy), ST_Within(crime_data_df[\"geom\"], community_income_df[\"comm_geometry\"])) \\\n",
    "        .groupBy(\"COMM\", \"comm_population\", \"comm_income\") \\\n",
    "        .agg(\n",
    "            count(col(\"*\")).alias(\"#crimes\"),\n",
    "        ) \\\n",
    "        .withColumn(\"#Crimes per capita\", col(\"#crimes\") / col(\"comm_population\")) \\\n",
    "        .withColumn(\"Income per capita\", col(\"comm_income\") / col(\"comm_population\")) \\\n",
    "        .select(\"COMM\", \"#Crimes per capita\", \"Income per capita\") \\\n",
    "        .orderBy(col(\"Income per capita\").desc())\n",
    "\n",
    "    community_income_and_crime_df.show()\n",
    "    return community_income_and_crime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7393da",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def query4(query3_result_df, crime_data_df, census_blocks_df, race_codes_df):\n",
    "    spark_session = crime_data_df.sql_ctx.sparkSession\n",
    "\n",
    "    top_3_areas = query3_result_df.orderBy(col(\"Income per capita\").desc()).limit(3)\n",
    "    bottom_3_areas = query3_result_df.orderBy(col(\"Income per capita\").asc()).limit(3)\n",
    "\n",
    "    top_comms = [row.COMM for row in top_3_areas.select(\"COMM\").collect()]\n",
    "    bottom_comms = [row.COMM for row in bottom_3_areas.select(\"COMM\").collect()]\n",
    "\n",
    "    crime_data_with_geom = crime_data_df.withColumn(\"crime_geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "    # Επιλογή μόνο των απαραίτητων στηλών από το census_blocks_df και broadcast\n",
    "    census_blocks_small = census_blocks_df.select(\"COMM\", \"geometry\")\n",
    "    crimes_with_comm = crime_data_with_geom.join(\n",
    "        broadcast(census_blocks_small),\n",
    "        ST_Within(col(\"crime_geom\"), col(\"geometry\")),\n",
    "        \"left\"\n",
    "    ).cache()  # caching, καθώς θα χρησιμοποιηθεί για δύο φίλτρα\n",
    "\n",
    "    # Φιλτράρισμα εγκλημάτων για τις Top και Bottom κοινότητες\n",
    "    top_crimes = crimes_with_comm.filter(col(\"COMM\").isin(top_comms))\n",
    "    bottom_crimes = crimes_with_comm.filter(col(\"COMM\").isin(bottom_comms))\n",
    "\n",
    "    race_mapping = {row[\"Vict Descent\"]: row[\"Vict Descent Full\"] for row in race_codes_df.collect()}\n",
    "    broadcast_race_mapping = spark_session.sparkContext.broadcast(race_mapping)\n",
    "\n",
    "    def map_race(code):\n",
    "        return broadcast_race_mapping.value.get(code, None)\n",
    "    map_race_udf = udf(map_race, StringType())\n",
    "\n",
    "    # Εφαρμογή του UDF στα DataFrame για να προσθέσουμε την πλήρη περιγραφή της φυλής\n",
    "    top_crimes_with_race = top_crimes.withColumn(\"Vict Descent Full\", map_race_udf(col(\"Vict Descent\")))\n",
    "    bottom_crimes_with_race = bottom_crimes.withColumn(\"Vict Descent Full\", map_race_udf(col(\"Vict Descent\")))\n",
    "\n",
    "    # Ομαδοποίηση και υπολογισμός αριθμού εγκλημάτων ανά race profile\n",
    "    top_grouped = top_crimes_with_race.groupBy(\"Vict Descent Full\") \\\n",
    "                                      .count() \\\n",
    "                                      .orderBy(col(\"count\").desc())\n",
    "    bottom_grouped = bottom_crimes_with_race.groupBy(\"Vict Descent Full\") \\\n",
    "                                            .count() \\\n",
    "                                            .orderBy(col(\"count\").desc())\n",
    "\n",
    "    print(\"Race profile για τις Top 3 Κοινότητες (ανάλογα με Income per capita):\")\n",
    "    top_grouped.show()\n",
    "\n",
    "    print(\"Race profile για τις Bottom 3 Κοινότητες (ανάλογα με Income per capita):\")\n",
    "    bottom_grouped.show()\n",
    "\n",
    "    return top_crimes_with_race, bottom_crimes_with_race\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41455709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a shared SparkContext\n",
    "conf = SparkConf().setAppName(\"SharedSparkContext\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "# Define configurations for Query 4:\n",
    "configs = [\n",
    "    {\"app_name\": \"Q4_CONFIG_1\", \"executor_cores\": 1, \"executor_memory\": \"2G\"},\n",
    "    {\"app_name\": \"Q4_CONFIG_2\", \"executor_cores\": 2, \"executor_memory\": \"4G\"},\n",
    "    {\"app_name\": \"Q4_CONFIG_3\", \"executor_cores\": 4, \"executor_memory\": \"8G\"}\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"Starting with SparkSession: {config['app_name']}\")\n",
    "    \n",
    "    spark_session = (\n",
    "        SparkSession(sc)\n",
    "        .newSession()\n",
    "        .builder\n",
    "        .appName(config[\"app_name\"])\n",
    "        .config(\"spark.executor.cores\", config[\"executor_cores\"])\n",
    "        .config(\"spark.executor.memory\", config[\"executor_memory\"])\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    income_df = load_income_df(spark_session)\n",
    "    census_blocks_df = load_census_blocks_df(spark_session)\n",
    "    crime_data_df = load_crime_data_df(spark_session)    \n",
    "    re_codes_df = load_re_codes_df(spark_session)\n",
    "\n",
    "    query3_result_df = query3(census_blocks_df, income_df, crime_data_df)\n",
    "\n",
    "    start_time = time.time()\n",
    "    top_crimes_with_race, bottom_crimes_with_race = query4(\n",
    "        query3_result_df, crime_data_df, census_blocks_df, re_codes_df\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Query 4 execution time for {config['app_name']}: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af863b7d",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f001539",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def query5(crime_df, police_stations_df):\n",
    "    police_stations_df = police_stations_df.withColumn(\"station_geom\", ST_Point(\"X\", \"Y\"))\n",
    "    crime_df = crime_df.filter((col(\"LAT\").isNotNull()) & (col(\"LON\").isNotNull()))\n",
    "    crime_df = crime_df.withColumn(\"crime_geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "    \n",
    "    distances_df = crime_df.crossJoin(police_stations_df) \\\n",
    "        .withColumn(\"distance\", ST_Distance(col(\"crime_geom\"), col(\"station_geom\")))\n",
    "\n",
    "    window_spec = Window.partitionBy(\"DR_NO\").orderBy(col(\"distance\"))\n",
    "    closest_stations_df = distances_df.withColumn(\"rank\", rank().over(window_spec)).filter(col(\"rank\") == 1)\n",
    "\n",
    "    query5_result_df = closest_stations_df.groupBy(\"DIVISION\") \\\n",
    "        .agg(\n",
    "            count(\"DR_NO\").alias(\"crime_count\"),\n",
    "            avg(\"distance\").alias(\"average_distance\")\n",
    "        ) \\\n",
    "        .orderBy(col(\"crime_count\").desc())\n",
    "\n",
    "    query5_result_df.select(\n",
    "        col(\"DIVISION\").alias(\"division\"),\n",
    "        col(\"average_distance\"),\n",
    "        col(\"crime_count\").alias(\"#\")\n",
    "    ).orderBy(col(\"#\").desc()).show()\n",
    "\n",
    "    return query5_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def query5_KNN(spark_session, crime_df):\n",
    "    SedonaRegistrator.registerAll(spark_session)\n",
    "    \n",
    "    police_stations_df = spark_session.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\",\n",
    "        header=True, inferSchema=True\n",
    "    )\n",
    "    police_stations_df = police_stations_df.withColumn(\"station_geom\", ST_Point(\"X\", \"Y\"))\n",
    "    \n",
    "    crime_df = crime_df.filter((col(\"LAT\").isNotNull()) & (col(\"LON\").isNotNull()))\n",
    "    crime_df = crime_df.withColumn(\"crime_geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "    \n",
    "    knn_df = ST_KNN(crime_df, police_stations_df, 1, False)\n",
    "    \n",
    "    # Υπολογισμός της απόστασης μεταξύ του σημείου του εγκλήματος και του πλησιέστερου σταθμού\n",
    "    knn_df = knn_df.withColumn(\"distance\", ST_Distance(col(\"crime_geom\"), col(\"station_geom\")))\n",
    "    \n",
    "    query5_result_df = knn_df.groupBy(\"DIVISION\") \\\n",
    "        .agg(\n",
    "            count(\"DR_NO\").alias(\"crime_count\"),\n",
    "            avg(\"distance\").alias(\"average_distance\")\n",
    "        ) \\\n",
    "        .orderBy(col(\"crime_count\").desc())\n",
    "    \n",
    "    query5_result_df.select(\n",
    "        col(\"DIVISION\").alias(\"division\"),\n",
    "        col(\"average_distance\"),\n",
    "        col(\"crime_count\").alias(\"#\")\n",
    "    ).orderBy(col(\"#\").desc()).show()\n",
    "    \n",
    "    return query5_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d26360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a shared SparkContext\n",
    "conf = SparkConf().setAppName(\"SharedSparkContext\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "# Workflow for each configuration\n",
    "configs = [\n",
    "    {\"app_name\": \"Q5_CONFIG_1\", \"num_executors\": 2, \"executor_cores\": 4, \"executor_memory\": \"8G\"},\n",
    "    {\"app_name\": \"Q5_CONFIG_2\", \"num_executors\": 4, \"executor_cores\": 2, \"executor_memory\": \"4G\"},\n",
    "    {\"app_name\": \"Q5_CONFIG_3\", \"num_executors\": 8, \"executor_cores\": 1, \"executor_memory\": \"2G\"},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    # Create a new SparkSession using the shared SparkContext and apply resource configurations\n",
    "    spark_session = (\n",
    "        SparkSession(sc)\n",
    "        .newSession()\n",
    "        .builder\n",
    "        .appName(config[\"app_name\"])\n",
    "        .config(\"spark.executor.instances\", config[\"num_executors\"])\n",
    "        .config(\"spark.executor.cores\", config[\"executor_cores\"])\n",
    "        .config(\"spark.executor.memory\", config[\"executor_memory\"])\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    # Load the crime dataset\n",
    "    crime_df = spark_session.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    \n",
    "    query5_result_df = query5(spark_session, crime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4268b524",
   "metadata": {},
   "source": [
    "# Main - Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e0044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Initializing spark session\")\n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"LosAngelesCrime\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    print(\"Loading crime data dataframe\")\n",
    "    crime_data_df = load_crime_data_df(spark_session)\n",
    "\n",
    "    # Q1\n",
    "    query1_dataframe(crime_data_df)\n",
    "    query1_rdd(crime_data_df)\n",
    "\n",
    "    # Q2\n",
    "    print(\"Running query 2 RDD\")\n",
    "    query2_rdd(crime_data_df)\n",
    "    print(\"Running query 2 SQL\")\n",
    "    query2_sql(spark_session, crime_data_df)\n",
    "    print(\"Saving crime data to a single parquet file\")\n",
    "    save_crime_data_to_single_parquet_file(crime_data_df)\n",
    "    print(\"Loading crime data from single parquet file\")\n",
    "    parquet_loaded_crime_data_df = load_crime_data_from_single_parquet_file(\n",
    "        spark_session)\n",
    "    parquet_loaded_crime_data_df = cleanup_crime_data(\n",
    "        parquet_loaded_crime_data_df)\n",
    "    print(\"Running query 2 RDD again with the dataframe being loaded from a single parquet file\")\n",
    "    query2_rdd(parquet_loaded_crime_data_df)\n",
    "\n",
    "    # Q3\n",
    "    print(\"Loading income dataframe\")\n",
    "    income_df = load_income_df(spark_session)\n",
    "    print(\"Loading census blocks dataframe\")\n",
    "    census_blocks_df = load_census_blocks_df(spark_session)\n",
    "    print(\"Running query 3 with default strategies\")\n",
    "    query3_result_df = query3(census_blocks_df, income_df, crime_data_df)\n",
    "    print(\"Explaining query 3 result with default strategies\")\n",
    "    query3_result_df.explain()\n",
    "    strategies = (\"BROADCAST\", \"MERGE\", \"SHUFFLE_HASH\", \"SHUFFLE_REPLICATE_NL\")\n",
    "    for (community_income_strategy, community_income_and_crime_strategy) in product(strategies, strategies):\n",
    "        print(f\"Running query 3 with community_income_strategy={community_income_strategy} and community_income_and_crime_strategy={community_income_and_crime_strategy}\")\n",
    "        res = query3_with_strategies(census_blocks_df, income_df, crime_data_df, community_income_and_crime_strategy, community_income_and_crime_strategy)\n",
    "        res.explain()\n",
    "    \n",
    "    \n",
    "    # Q4\n",
    "    print(\"Running query 4\")\n",
    "    re_codes_df = load_re_codes_df(spark_session)\n",
    "    query4(query3_result_df, crime_data_df, re_codes_df)\n",
    "    \n",
    "  # Q5\n",
    "    print(\"Running query 5\") \n",
    "    police_stations_df = load_police_stations_df(spark_session)\n",
    "    query5_result_df = query5(crime_data_df, police_stations_df)\n",
    "    query5_result_df.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4438ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
